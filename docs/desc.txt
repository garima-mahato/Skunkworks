The OSCAR pretraining method which I have to re-implement is mainly based on making the the weakly-supervised task of aligning image's semantics with its corresponding text semantics. There are majorly 2 problems with earlier implementations. First, the object/region features are extracted using object detectors in which the regions are over-sampled like in Faster RCNN. So, the extracted regions might contain portions of other objects implying that each of these feature contain a mix of features of several objects. This introduces ambiguity in the data itself. Since there are no explicit labeled alignments between objects or regions of the image and the words within the caption to match the semantics in both visual and language mode. It is difficult for the model to learn this alignment by itself. While going through several datasets of VL, it was observed that the main objects of any image are usually mentioned in the text. For example, on the MS COCO dataset, the percentages that an image and its paired text share at least 1, 2, 3 objects are 49:7%, 22:2%, 12:9%, respectively. This led to an idea of using these main objects for aligning image data and text data. Since these main objects act as anchors around which rest of the objects or regions within an image align with the rest of the words in the text, these are known as anchor points.
These acts as labels for the alignment problems and hence the input now sent for pre-training consists of 3 parts: word embedding of caption, word embedding of object tags and region vectors of image. Word embedding for caption having the same dimension as BERT input found out. Image with K regions is passed through Faster-RCNN which results in 2048-dimensional vector of region feature and 4-dimensional vector of region position for each of the regions. These 2 vectors are concatenated to form position sensitive region feature vector. This passed through a linear layer to transform into vector of same dimension as word embedding. This same Faster-RCNN is also used to detect object tags which are transformed into word embedding of same size as other embeddings. BERT variant is used as model for training. The loss function used is a combination of 2 loss functions: masked token loss and contrastive loss. Object tags help the self-attention layers of the model to learn the semantics faster as the attention value for the region feature will be high for the words similar in semantics with the object tags.